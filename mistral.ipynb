{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Install required packages\n",
        "! pip install --upgrade nltk\n",
        "! pip install openai evaluate transformers datasets pandarallel tiktoken\n",
        "! pip install -U flash-attn --no-build-isolation"
      ],
      "metadata": {
        "id": "N8QeXJ-16vc_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "RDxdyEfJ6FXO"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tiktoken\n",
        "import time\n",
        "import random\n",
        "import json\n",
        "import textwrap\n",
        "from huggingface_hub import login\n",
        "from pathlib import Path\n",
        "from sklearn import metrics\n",
        "from google.colab import drive, userdata\n",
        "from transformers import AutoConfig, AutoModelForCausalLM, AutoTokenizer"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "login(token=userdata.get('HF_TOKEN'))"
      ],
      "metadata": {
        "id": "lhRO_AqPAplq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "!ls \"/content/drive/My Drive\""
      ],
      "metadata": {
        "id": "A1XlFmt262jU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set required variables\n",
        "dataset_folder_root = Path('/content/drive/My Drive/DP/datasets/csfd')\n",
        "\n",
        "train_file_path = dataset_folder_root / 'train_top5_withoutOOC.csv'\n",
        "test_file_path = dataset_folder_root / 'test_top5_withoutOOC.csv'\n",
        "\n",
        "model_name = \"mistralai/Mistral-7B-Instruct-v0.2\""
      ],
      "metadata": {
        "id": "orV784he66Du"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16, attn_implementation=\"flash_attention_2\", device_map=\"auto\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)"
      ],
      "metadata": {
        "id": "w5e1RtbbvXA1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def count_tokens(text: str) -> int:\n",
        "  encoding = tokenizer(text)\n",
        "  return len(encoding.input_ids)\n",
        "\n",
        "\n",
        "def is_token_count_valid(text: str) -> bool:\n",
        "  count = count_tokens(text)\n",
        "  return count > 56 and count < 512\n",
        "\n",
        "def compute_metrics(ground_truth, predicted):\n",
        "  acc = metrics.accuracy_score(ground_truth, predicted)\n",
        "  f1 = metrics.f1_score(ground_truth, predicted, average='macro', zero_division=0)\n",
        "  precision = metrics.precision_score(ground_truth, predicted, average='macro', zero_division=0)\n",
        "  recall = metrics.recall_score(ground_truth, predicted, average='macro', zero_division=0)\n",
        "\n",
        "  return {\n",
        "      'accuracy': acc * 100,\n",
        "      'f1': f1 * 100,\n",
        "      'precision': precision * 100,\n",
        "      'recall': recall * 100\n",
        "  }\n",
        "\n",
        "def evaluate_mistral(results: list[pd.DataFrame]):\n",
        "  acc_list, f1_list, precision_list, recall_list = [], [], [], []\n",
        "  for rep_df in results:\n",
        "    rep_metrics = compute_metrics(rep_df['label'], rep_df['answer'])\n",
        "    acc_list.append(rep_metrics['accuracy'])\n",
        "    f1_list.append(rep_metrics['f1'])\n",
        "    precision_list.append(rep_metrics['precision'])\n",
        "    recall_list.append(rep_metrics['recall'])\n",
        "\n",
        "  avg = (round(np.mean(acc_list), 2), round(np.mean(f1_list), 2), round(np.mean(precision_list), 2), round(np.mean(recall_list), 2))\n",
        "  std = (round(np.std(acc_list), 2), round(np.std(f1_list), 2), round(np.std(precision_list), 2), round(np.std(recall_list), 2))\n",
        "  return avg, std\n",
        "\n",
        "\n",
        "def extract_samples(df: pd.DataFrame) -> pd.DataFrame:\n",
        "  author_names = df['label'].unique().tolist()\n",
        "  result = pd.DataFrame(columns=['label', 'query_text', 'example_text'])\n",
        "  for author in author_names:\n",
        "    text_1, text_2 = df[df['label'] == author]['text'].sample(2)  # Get random 2 text from this author\n",
        "    result = pd.concat([result, pd.DataFrame([[author, text_1, text_2]], columns=result.columns)], ignore_index=True)\n",
        "    result = result.sort_values(by=['label'])\n",
        "    result = result.reset_index(drop=True)\n",
        "  return result\n",
        "\n",
        "def create_system_prompt() -> str:\n",
        "  return textwrap.dedent(\"\"\"\\\n",
        "    Odpověz pomocí JSON objektu, který obsahuje dva prvky:\n",
        "    {\n",
        "      \"analysis\": Odůvodnění tvojí odpovědi.\n",
        "      \"answer\": ID autora analyzovaného textu.\n",
        "    }\"\"\"\n",
        "  )\n",
        "\n",
        "  # return textwrap.dedent(\"\"\"\\\n",
        "  #   Respond with a JSON object including two key elements:\n",
        "  #   {\n",
        "  #     \"analysis\": Reasoning behind your answer.\n",
        "  #     \"answer\": The query text's author ID.\n",
        "  #   }\"\"\"\n",
        "  # )\n",
        "\n",
        "def create_prompt(query: str, examples: str) -> str:\n",
        "  return \"S ohledem na sadu textů se známými autory a analyzovaný text \" + \\\n",
        "         \"urči autora analyzovaného textu. Analyzuj styly psaní \" + \\\n",
        "         \"vstupních textů, přičemž ignoruj rozdíly v tématu a obsahu. \" + \\\n",
        "         \"Zaměř se na jazykové rysy, jako jsou interpunkce, vzácná \" + \\\n",
        "         \"slova, přípony, kvantifikátory, \" + \\\n",
        "         \"humor, sarkasmus, typografické chyby a překlepy. Vstupní texty \" + \\\n",
        "         \"jsou ohraničeny trojitými zpětnými apostrofy. ```\\n\\n\" + \\\n",
        "         f\"Analyzovaný text: {query}\\n\\n\" + \\\n",
        "         f\"Texty od potenciálních autorů: {examples}\\n```\"\n",
        "\n",
        "  # return \"Given a set of texts with known authors and a query text, \" + \\\n",
        "  #        \"determine the author of the query text. Analyze the writing \" + \\\n",
        "  #        \"styles of the input texts, disregarding the differences in \" + \\\n",
        "  #        \"topic and content. Focus on linguistic features such as phrasal \" + \\\n",
        "  #        \"verbs, modal verbs, punctuation, rare words, affixes, quantities, \" + \\\n",
        "  #        \"humor, sarcasm, typographical errors, and misspellings. \" + \\\n",
        "  #        \"The input texts are delimited with triple backticks. ```\\n\\n\" + \\\n",
        "  #        f\"Query text: {query}\\n\\n\" + \\\n",
        "  #        f\"Texts from potential authors: {examples}\\n\\n```\"\n"
      ],
      "metadata": {
        "id": "p7FlcgXoOFca"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create dataset suitable for prompts\n",
        "df = pd.read_csv(train_file_path)\n",
        "df = df[[\"label\", \"text\"]]\n",
        "\n",
        "print('Original shape:', df.shape)\n",
        "df = df[df[\"text\"].apply(lambda x: is_token_count_valid(x))]\n",
        "print('Shape after filtering:', df.shape)"
      ],
      "metadata": {
        "id": "qhh-4eb9NCsY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "retry_count = 3\n",
        "retry_delay = 60\n",
        "\n",
        "reps = 3\n",
        "rep_responses = []\n",
        "\n",
        "for _ in range(reps):\n",
        "  responses = []\n",
        "\n",
        "  samples = extract_samples(df)\n",
        "  queries = samples['query_text'].tolist()\n",
        "\n",
        "  example_texts = {row['label']: row['example_text'] for _, row in samples.iterrows()}\n",
        "  examples = json.dumps(example_texts, ensure_ascii=False)\n",
        "\n",
        "  for query in queries:\n",
        "    messages = [\n",
        "      {\"role\": \"system\", \"content\": create_prompt(query, examples)},\n",
        "      {\"role\": \"user\", \"content\": create_system_prompt()}\n",
        "    ]\n",
        "    model_inputs = tokenizer.apply_chat_template(messages, return_tensors=\"pt\").to(\"cuda\")\n",
        "    generated_ids = model.generate(model_inputs, top_p=1.0, max_new_tokens=4096, do_sample=False, pad_token_id=tokenizer.eos_token_id)\n",
        "    response_str = tokenizer.batch_decode(generated_ids)[0]\n",
        "\n",
        "    correct_label = samples[samples['query_text'] == query]['label']['label']\n",
        "    print(f'Response:\\n{response_str}\\n')\n",
        "    print(f'Correct label: {correct_label}')\n",
        "\n",
        "    try:\n",
        "      response = json.loads(response_str, strict=False)\n",
        "    except json.JSONDecodeError:\n",
        "      print(\"Error while decoding response.\")\n",
        "      response = json.loads(\"{}\")\n",
        "      response['analysis'] = response_str\n",
        "      response['answer'] = \"error\"\n",
        "\n",
        "    response[\"query_text\"] = query\n",
        "    response[\"example_texts\"] = examples\n",
        "    response[\"label\"] = correct_label\n",
        "    responses.append(response)\n",
        "\n",
        "  rep_responses.append(pd.DataFrame(responses))\n"
      ],
      "metadata": {
        "id": "MlVx8tkgiBIb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "avg, std = evaluate_mistral(rep_responses)\n",
        "\n",
        "print('AVG:')\n",
        "print(f'  acc: {avg[0]}, f1: {avg[1]}, precision: {avg[2]}, recall: {avg[3]}')\n",
        "print('STD:')\n",
        "print(f'  acc: {std[0]}, f1: {std[1]}, precision: {std[2]}, recall: {std[3]}')\n"
      ],
      "metadata": {
        "id": "oGqHAwdQ73q9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}